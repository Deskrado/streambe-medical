model_name: "Qwen/Qwen2.5-7B"
tokenized_dataset_path: "data/tokenized/sft_train"
val_dataset_path: "data/tokenized/sft_val"

output_dir: "output/sft_qwen3"
logging_steps: 20
save_steps: 500
eval_steps: 500
num_train_epochs: 2
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
warmup_ratio: 0.03
learning_rate: 2e-5
lr_scheduler_type: "cosine"
fp16: true
bf16: false
gradient_checkpointing: true
report_to: ["wandb"]
