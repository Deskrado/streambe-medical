model_name: "Qwen/Qwen2.5-7B"
tokenized_dataset_path: "data/tokenized/pretrain"

output_dir: "output/pretrain_qwen3"
num_train_epochs: 1
batch_size: 2
gradient_accumulation_steps: 16
learning_rate: 1e-4
warmup_ratio: 0.05
logging_steps: 20
save_steps: 500
fp16: true
gradient_checkpointing: true
report_to: ["wandb"]
